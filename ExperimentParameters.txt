Parameter Selection
To ensure the optimal performance of GAN-BERT, we carried out experiments to fine-tune key parameters. These parameters play a critical role in balancing computational efficiency, training stability, and model accuracy.

Parameters Used by GAN:
- num_hidden_layers_g: Specifies the number of hidden layers in the generator, affecting its ability to model complex data distributions.
- num_hidden_layers_d: Defines the number of hidden layers in the discriminator, influencing its ability to distinguish real from pseudo-labeled data.
- noise_size: Indicates the dimensionality of the noise vector input to the generator, crucial for producing diverse pseudo-labeled examples.
- out_dropout_rate: Sets the dropout rate applied to the discriminator’s inputs to mitigate overfitting.
- learning_rate_discriminator: Learning rate for optimizing the discriminator, balancing its training relative to the generator.
- learning_rate_generator: Learning rate for optimizing the generator, ensuring stable adversarial learning dynamics.
- epsilon: A small constant added for numerical stability during optimization.
- num_train_epochs: Total number of epochs for GAN-specific training iterations.
- apply_scheduler: Determines whether a learning rate scheduler is employed during training.
- warmup_proportion: Defines the fraction of training steps allocated for learning rate warm-up, enhancing optimization convergence.
- multi_gpu: Enables distributed training across multiple GPUs for scalability and efficiency.
- apply_balance: Ensures balanced representation of labeled data by replicating minority class samples during training.
- label_mask_rate: Controls the proportion of labeled samples masked to simulate semi-supervised learning conditions.
- d_loss: Specifies the loss function used for the discriminator.
- g_loss: Specifies the loss function used for the generator.

Parameters Used by BERT:
- max_seq_length: Sets the maximum number of tokens processed per input sequence, balancing contextual understanding with computational efficiency.
- batch_size: Determines the number of samples processed simultaneously during training.
- tokenizer: Converts raw text into token IDs using a pretrained tokenization schema.
- hidden_size: Specifies the dimensionality of hidden layers within the BERT architecture.
- transformer: Indicates the specific pretrained BERT model variant employed (e.g., bert-base-uncased).
- add_special_tokens: Includes special tokens (e.g., [CLS], [SEP]) in input sequences for improved contextual segmentation.
- padding: Controls sequence padding to match max_seq_length, ensuring uniform input dimensions.
- truncation: Trims sequences exceeding the max_seq_length, preserving computational resources.
- train_examples, test_examples: Specifies datasets utilized for training and evaluation.
- attention_mask: Binary mask indicating token relevance within sequences, aiding in contextual focus.
- label2one_hot: Converts class labels into one-hot encoded vectors for multi-class classification tasks.
- D_real_logits, D_fake_logits: Logits generated by BERT, serving as input for the discriminator to evaluate real and pseudo-labeled examples.

Shared Parameters Between GAN and BERT:
- hidden_size: Aligns the dimensionality of BERT hidden representations with the discriminator’s input size, facilitating effective information exchange.
- dropout_rate: Mitigates overfitting by applying regularization across both BERT and the discriminator.
- scheduler_d and scheduler_g: Learning rate schedulers applied to the discriminator and generator, respectively, to stabilize the training dynamics.

Experiment 1: GAN-BERT Standardization
This experiment evaluated GAN-BERT’s performance with a minimal labeled dataset of 10 auto-annotated examples per label. The goal was to assess its ability to generalize and classify sensitive information with constrained data. The labeled dataset was based on the TREC Question Classification dataset (https://huggingface.co/datasets/CogComp/trec), including labels NUM:num, DESC:desc, HUM:hum, ENTY:enty, and LOC:loc. To ensure fairness, records were balanced across categories, and the same process was applied to the test dataset.

The unlabeled dataset consisted of 5,343 records labeled as UNK:UNK, providing various queries for pseudo-labeled data generation through GAN-BERT’s generative component. The model operated in a semi-supervised mode, leveraging both labeled and pseudo-labeled data.

Key parameters used in this experiment:
- max_seq_length: 64
- learning_rate: 5e-5
- batch_size: 64
- num_train_epochs: 10
- out_dropout_rate: 0.1

The model's performance at the end of training:
- Accuracy: 94%
- Precision: 94%
- Recall: 84%
- F1-Score: 84%

These results highlight GAN-BERT’s effectiveness and adaptability in semi-supervised learning tasks for sensitive information classification.

Experiment 2: BERT with Minimal Labels
This experiment assessed the performance of a pretrained BERT model fine-tuned on a minimal dataset of 10 manually annotated examples. The primary goal was to evaluate its ability to generalize and classify sensitive information when trained on limited labeled data.

Dataset Preparation:
A subset of 10 labeled examples was extracted from the Monsanto dataset, ensuring representation across all categories, including: GHOST: ghost, TOXIC: toxic, CHEMI: chemi, REGUL: reg. A separate test dataset of 76 examples from the Monsanto dataset was used for the evaluation, ensuring a balanced representation across categories. The unlabeled Enron dataset, comprising 5,340 entries, was used to simulate real-world unstructured data and enhance the semi-supervised training setup.

Model Configuration:
- Maximum Sequence Length: 64
- Batch Size: 64
- Generator Hidden Layers: 1
- Discriminator Hidden Layers: 1
- Noise Vector Size: 100
- Discriminator Input Dropout Rate: 0.2
- Data Balance: True 
- Learning Rate: 5e-5
- Training Epochs: 10
- Warmup Proportion: 0.1

The model's performance improved across epochs, achieving the following metrics at the end of training:
- Accuracy: 50.0%
- Precision: 22.3%
- Recall: 25.9%
- F1-Score: 23.4%

Experiment 3: GAN-BERT with Minimal Labels
This experiment evaluated the performance of GAN-BERT using the same minimal labeled dataset as Experiment 2, allowing a direct comparison with BERT. The primary goal was to evaluate the impact of GAN-BERT’s semi-supervised learning capabilities on accuracy, robustness, and efficiency when trained with minimal labeled data.

Dataset Preparation:
The labeled dataset consisted of 10 manually annotated examples from the Monsanto dataset, distributed across the following main categories: GHOST:ghost, TOXIC:toxic, CHEMI:chemi, and REGUL:regul. The Enron dataset, with 5,340 unlabeled entries, was used as the source for pseudolabeled data generation by GAN-BERT's generator. A balanced test dataset containing 76 entries from the Monsanto dataset was used for the evaluation.

Model Configuration:
- Maximum Sequence Length: 64
- Batch Size: 64
- Generator Hidden Layers: 1
- Discriminator Hidden Layers: 1
- Noise Vector Size: 100
- Discriminator Input Dropout Rate: 0.2
- Data Balance: True 
- Learning Rate: 5e-5
- Training Epochs: 10
- Warmup Proportion: 0.1

The model's performance at the end of training:
- Accuracy: 61.8%
- Precision: 65.2%
- Recall: 61.8%
- F1-Score: 60.4%
