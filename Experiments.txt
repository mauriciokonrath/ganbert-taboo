Parameter Selection
To ensure the optimal performance of GAN-BERT, we carried out experiments to fine-tune key parameters. These parameters play a critical role in balancing computational efficiency, training stability, and model accuracy.

Parameters Used by GAN:
- num_hidden_layers_g: Specifies the number of hidden layers in the generator, affecting its ability to model complex data distributions.
- num_hidden_layers_d: Defines the number of hidden layers in the discriminator, influencing its ability to distinguish real from pseudo-labeled data.
- noise_size: Indicates the dimensionality of the noise vector input to the generator, crucial for producing diverse pseudo-labeled examples.
- out_dropout_rate: Sets the dropout rate applied to the discriminator’s inputs to mitigate overfitting.
- learning_rate_discriminator: Learning rate for optimizing the discriminator, balancing its training relative to the generator.
- learning_rate_generator: Learning rate for optimizing the generator, ensuring stable adversarial learning dynamics.
- epsilon: A small constant added for numerical stability during optimization.
- num_train_epochs: Total number of epochs for GAN-specific training iterations.
- apply_scheduler: Determines whether a learning rate scheduler is employed during training.
- warmup_proportion: Defines the fraction of training steps allocated for learning rate warm-up, enhancing optimization convergence.
- multi_gpu: Enables distributed training across multiple GPUs for scalability and efficiency.
- apply_balance: Ensures balanced representation of labeled data by replicating minority class samples during training.
- label_mask_rate: Controls the proportion of labeled samples masked to simulate semi-supervised learning conditions.
- d_loss: Specifies the loss function used for the discriminator.
- g_loss: Specifies the loss function used for the generator.

Parameters Used by BERT:
- max_seq_length: Sets the maximum number of tokens processed per input sequence, balancing contextual understanding with computational efficiency.
- batch_size: Determines the number of samples processed simultaneously during training.
- tokenizer: Converts raw text into token IDs using a pretrained tokenization schema.
- hidden_size: Specifies the dimensionality of hidden layers within the BERT architecture.
- transformer: Indicates the specific pretrained BERT model variant employed (e.g., bert-base-uncased).
- add_special_tokens: Includes special tokens (e.g., [CLS], [SEP]) in input sequences for improved contextual segmentation.
- padding: Controls sequence padding to match max_seq_length, ensuring uniform input dimensions.
- truncation: Trims sequences exceeding the max_seq_length, preserving computational resources.
- train_examples, test_examples: Specifies datasets utilized for training and evaluation.
- attention_mask: Binary mask indicating token relevance within sequences, aiding in contextual focus.
- label2one_hot: Converts class labels into one-hot encoded vectors for multi-class classification tasks.
- D_real_logits, D_fake_logits: Logits generated by BERT, serving as input for the discriminator to evaluate real and pseudo-labeled examples.

Shared Parameters Between GAN and BERT:
- hidden_size: Aligns the dimensionality of BERT hidden representations with the discriminator’s input size, facilitating effective information exchange.
- dropout_rate: Mitigates overfitting by applying regularization across both BERT and the discriminator.
- scheduler_d and scheduler_g: Learning rate schedulers applied to the discriminator and generator, respectively, to stabilize the training dynamics.

Experiments

Experiment 1

Configuration
Transformer Parameters:
- max_seq_length: 64
- batch_size: 64

GAN-BERT Specific Parameters
- num_hidden_layers_g (Generator): 1
- num_hidden_layers_d (Discriminator): 1
- noise_size: 100
- out_dropout_rate: 0.1
- apply_balance: True

Optimization Parameters
- learning_rate (Discriminator): 5e-5
- learning_rate (Generator): 5e-5
- epsilon: 1e-8
- num_train_epochs: 30
- multi_gpu: True
- apply_scheduler: False
- warmup_proportion: 0.1
- print_each_n_step: 10

---

Experiment 2

Configuration
Transformer Parameters:
- max_seq_length: 256
- batch_size: 64

GAN-BERT Specific Parameters:
- num_hidden_layers_g (Generator): 1
- num_hidden_layers_d (Discriminator): 1
- noise_size: 100
- out_dropout_rate: 0.1
- apply_balance: True

Optimization Parameters:
- learning_rate (Discriminator): 5e-6
- learning_rate (Generator): 5e-6
- epsilon: 1e-8
- num_train_epochs: 30
- multi_gpu: True
- apply_scheduler: False
- warmup_proportion: 0.1
- print_each_n_step: 10

---

Experiment 3

Configuration
Transformer Parameters:
- max_seq_length: 64
- batch_size: 64

GAN-BERT Specific Parameters:
- num_hidden_layers_g (Generator): 1
- num_hidden_layers_d (Discriminator): 1
- noise_size: 100
- out_dropout_rate: 0.1
- apply_balance: True

Optimization Parameters:
- learning_rate (Discriminator): 5e-5
- learning_rate (Generator): 5e-5
- epsilon: 1e-8
- num_train_epochs: 30
- multi_gpu: True
- apply_scheduler: True
- warmup_proportion: 0.1
- print_each_n_step: 10

---

Experiment 4

Configuration
Transformer Parameters:
- max_seq_length: 256
- batch_size: 16

GAN-BERT Specific Parameters:
- num_hidden_layers_g (Generator): 3
- num_hidden_layers_d (Discriminator): 2
- noise_size: 128
- out_dropout_rate: 0.2
- apply_balance: True

Optimization Parameters:
- learning_rate (Discriminator): 3e-5
- learning_rate (Generator): 3e-5
- epsilon: 1e-8
- num_train_epochs: 30
- multi_gpu: True
- apply_scheduler: False
- warmup_proportion: 0.15
- print_each_n_step: 10

---

Experiment 5

Configuration
Transformer Parameters:
- max_seq_length: 64
- batch_size: 64

GAN-BERT Specific Parameters:
- num_hidden_layers_g (Generator): 1
- num_hidden_layers_d (Discriminator): 1
- noise_size: 100
- out_dropout_rate: 0.2
- apply_balance: True

Optimization Parameters:
- learning_rate (Discriminator): 5e-5
- learning_rate (Generator): 5e-5
- epsilon: 1e-8
- num_train_epochs: 30
- multi_gpu: True
- apply_scheduler: False
- warmup_proportion: 0.1
- print_each_n_step: 10

