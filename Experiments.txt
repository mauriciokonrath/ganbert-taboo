Parameter Selection
To ensure the optimal performance of GAN-BERT, we carried out experiments to fine-tune key parameters. These parameters play a critical role in balancing computational efficiency, training stability, and model accuracy.

Parameters Used by GAN:
- num_hidden_layers_g: Specifies the number of hidden layers in the generator, affecting its ability to model complex data distributions.
- num_hidden_layers_d: Defines the number of hidden layers in the discriminator, influencing its ability to distinguish real from pseudo-labeled data.
- noise_size: Indicates the dimensionality of the noise vector input to the generator, crucial for producing diverse pseudo-labeled examples.
- out_dropout_rate: Sets the dropout rate applied to the discriminator’s inputs to mitigate overfitting.
- learning_rate_discriminator: Learning rate for optimizing the discriminator, balancing its training relative to the generator.
- learning_rate_generator: Learning rate for optimizing the generator, ensuring stable adversarial learning dynamics.
- epsilon: A small constant added for numerical stability during optimization.
- num_train_epochs: Total number of epochs for GAN-specific training iterations.
- apply_scheduler: Determines whether a learning rate scheduler is employed during training.
- warmup_proportion: Defines the fraction of training steps allocated for learning rate warm-up, enhancing optimization convergence.
- multi_gpu: Enables distributed training across multiple GPUs for scalability and efficiency.
- apply_balance: Ensures balanced representation of labeled data by replicating minority class samples during training.
- label_mask_rate: Controls the proportion of labeled samples masked to simulate semi-supervised learning conditions.
- d_loss: Specifies the loss function used for the discriminator.
- g_loss: Specifies the loss function used for the generator.

Parameters Used by BERT:
- max_seq_length: Sets the maximum number of tokens processed per input sequence, balancing contextual understanding with computational efficiency.
- batch_size: Determines the number of samples processed simultaneously during training.
- tokenizer: Converts raw text into token IDs using a pretrained tokenization schema.
- hidden_size: Specifies the dimensionality of hidden layers within the BERT architecture.
- transformer: Indicates the specific pretrained BERT model variant employed (e.g., bert-base-uncased).
- add_special_tokens: Includes special tokens (e.g., [CLS], [SEP]) in input sequences for improved contextual segmentation.
- padding: Controls sequence padding to match max_seq_length, ensuring uniform input dimensions.
- truncation: Trims sequences exceeding the max_seq_length, preserving computational resources.
- train_examples, test_examples: Specifies datasets utilized for training and evaluation.
- attention_mask: Binary mask indicating token relevance within sequences, aiding in contextual focus.
- label2one_hot: Converts class labels into one-hot encoded vectors for multi-class classification tasks.
- D_real_logits, D_fake_logits: Logits generated by BERT, serving as input for the discriminator to evaluate real and pseudo-labeled examples.

Shared Parameters Between GAN and BERT:
- hidden_size: Aligns the dimensionality of BERT hidden representations with the discriminator’s input size, facilitating effective information exchange.
- dropout_rate: Mitigates overfitting by applying regularization across both BERT and the discriminator.
- scheduler_d and scheduler_g: Learning rate schedulers applied to the discriminator and generator, respectively, to stabilize the training dynamics.

