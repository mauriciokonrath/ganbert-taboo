{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/ganbert-pytorch/blob/main/GANBERT_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT (in Pytorch and compatible with HuggingFace)\n",
        "\n",
        "This is a Pytorch (+ **Huggingface** transformers) implementation of the GAN-BERT model from https://github.com/crux82/ganbert. While the original GAN-BERT was an extension of BERT, this implementation can be adapted to several architectures, ranging from Roberta to Albert!\n",
        "\n",
        "**NOTE**: given that this implementation is different from the original one in Tensorflow, some results can be slighty different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0m5KR34gmRH"
      },
      "source": [
        "Let's GO!\n",
        "\n",
        "Required Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "b1545b61-92aa-4c83-bdde-497d1f4f79d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.3.2\n",
            "  Using cached transformers-4.3.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.3.2)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.3.2)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2024.12.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (1.4.2)\n",
            "Using cached transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Baixar o recurso de pontuação da NLTK (apenas na primeira vez)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "eaeae7a0-189e-4923-9475-95851d4c7a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw0HC_hU3FUy",
        "outputId": "9d44f848-629b-466c-c710-c45d718cad9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ganbert' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64  #comprimento máximo de sequência de tokens\n",
        "batch_size = 64 #número de exemplos de treinamento que serão processados\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator,\n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1;\n",
        "# number of hidden layers in the discriminator,\n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1;\n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.1\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets,\n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8 #evita divisão por zero\n",
        "num_train_epochs = 30 #número de épocas para treinar o modelo\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = True\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\"\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "#model_name = \"amazon/bort\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "! git clone https://github.com/mauriciokonrath/ganbert.git\n",
        "\n",
        "#  NOTE: in this setting 50 classes are involved\n",
        "labeled_file = \"./ganbert/data/standardized_labeled_monsanto_withoutSub.tsv\"\n",
        "unlabeled_file = \"./ganbert/data/unlabeled_enron_5000.tsv\"\n",
        "test_filename = \"./ganbert/data/standardized_test_monsanto_withoutSub.tsv\"\n",
        "\n",
        "#categorias de rótulos que o modelo deve aprender a classificar.\n",
        "#categorias de rótulos que o modelo deve aprender a classificar.\n",
        "label_list = [\"UNK_UNK\",\"GHOST_ghost\", \"TOXIC_toxic\",\n",
        "              \"CHEMI_chemi\", \"REGUL_regul\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxghkkZq3Gbn",
        "outputId": "4a00db68-2f3c-4c01-f7b6-7ef8cb972e6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7cP8q7K3BId"
      },
      "outputs": [],
      "source": [
        "#ler um arquivo de texto contendo dados de perguntas e criar exemplos de treinamento ou desenvolvimento\n",
        "def get_qc_examples(input_file):\n",
        "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "  examples = []\n",
        "\n",
        "  with open(input_file, 'r') as f:\n",
        "      contents = f.read()\n",
        "      file_as_list = contents.splitlines()\n",
        "      for line in file_as_list[1:]:\n",
        "          split = line.split(\" \")\n",
        "          question = ' '.join(split[1:])\n",
        "\n",
        "          text_a = question\n",
        "          inn_split = split[0].split(\":\")\n",
        "          label = inn_split[0] + \"_\" + inn_split[1]\n",
        "          examples.append((text_a, label))\n",
        "      f.close()\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43tOavNqib4"
      },
      "source": [
        "**Load** the input QC dataset (fine-grained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXCwFyF2qhw7"
      },
      "outputs": [],
      "source": [
        "#Load the examples\n",
        "labeled_examples = get_qc_examples(labeled_file)\n",
        "unlabeled_examples = get_qc_examples(unlabeled_file)\n",
        "test_examples = get_qc_examples(test_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "outputs": [],
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are\n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples\n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask:\n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples):\n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "\n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization\n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "\n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset),\n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCRJWsuxjGqb"
      },
      "source": [
        "Prepares input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WINaaC0KjIDx"
      },
      "outputs": [],
      "source": [
        "#MODIFICADO\n",
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are\n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples\n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask:\n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples):\n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "\n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization\n",
        "  for (text, label_mask) in examples:\n",
        "    # Clean up the label by removing extraneous characters and text\n",
        "    label = text[1].split('\\t')[0]\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "    # Check if the label is in the label_map\n",
        "    if label in label_map:\n",
        "      label_id_array.append(label_map[label])\n",
        "    else:\n",
        "      # Handle the case where the label is not in label_map\n",
        "      # Here we assign the label 'UNK_UNK' if not found\n",
        "      label_id_array.append(label_map['UNK_UNK'])\n",
        "\n",
        "    label_mask_array.append(label_mask)\n",
        "\n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset),\n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c-nsMXlKX-D",
        "outputId": "bb3418f6-1cf6-466f-c87b-b4cc5df25663"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-b6233fd3d80c>:66: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  label_mask_array = torch.tensor(label_mask_array)\n"
          ]
        }
      ],
      "source": [
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = labeled_examples\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "#If unlabel examples are available\n",
        "if unlabeled_examples:\n",
        "  train_examples = train_examples + unlabeled_examples\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "#------------------------------\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ihcw3vquaQm"
      },
      "source": [
        "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "outputs": [],
      "source": [
        "#------------------------------\n",
        "#   The Generator as in\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uje9s2zQunFc"
      },
      "source": [
        "We instantiate the Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylz5rvqE3U2S",
        "outputId": "cfee5476-0c63-4fd0-fea3-e8dd6ad9b6b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The config file is required to get the dimension of the vector produced by\n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "# print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3qzp2-usZE"
      },
      "source": [
        "Let's go with the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhqylHGK3Va4",
        "outputId": "d2a83f75-9873-437a-fb20-574f85a57bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.73s.\n",
            "  Batch 20  of  88.    Elapsed: 9.5s.\n",
            "  Batch 30  of  88.    Elapsed: 14.31s.\n",
            "  Batch 40  of  88.    Elapsed: 19.1s.\n",
            "  Batch 50  of  88.    Elapsed: 23.84s.\n",
            "  Batch 60  of  88.    Elapsed: 28.55s.\n",
            "  Batch 70  of  88.    Elapsed: 33.22s.\n",
            "  Batch 80  of  88.    Elapsed: 37.88s.\n",
            "  Average training loss generator: 0.312\n",
            "  Average training loss discriminator: 3.174\n",
            "  Training epoch took: 41.5s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.368\n",
            "  Test Loss: 1.381\n",
            "  Test took: 41.63s\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.61s.\n",
            "  Batch 20  of  88.    Elapsed: 9.21s.\n",
            "  Batch 30  of  88.    Elapsed: 13.8s.\n",
            "  Batch 40  of  88.    Elapsed: 18.4s.\n",
            "  Batch 50  of  88.    Elapsed: 22.99s.\n",
            "  Batch 60  of  88.    Elapsed: 27.59s.\n",
            "  Batch 70  of  88.    Elapsed: 32.19s.\n",
            "  Batch 80  of  88.    Elapsed: 36.79s.\n",
            "  Average training loss generator: 0.696\n",
            "  Average training loss discriminator: 1.528\n",
            "  Training epoch took: 40.41s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.500\n",
            "  Test Loss: 2.070\n",
            "  Test took: 40.53s\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.66s.\n",
            "  Batch 20  of  88.    Elapsed: 9.35s.\n",
            "  Batch 30  of  88.    Elapsed: 14.06s.\n",
            "  Batch 40  of  88.    Elapsed: 18.77s.\n",
            "  Batch 50  of  88.    Elapsed: 23.48s.\n",
            "  Batch 60  of  88.    Elapsed: 28.16s.\n",
            "  Batch 70  of  88.    Elapsed: 32.83s.\n",
            "  Batch 80  of  88.    Elapsed: 37.49s.\n",
            "  Average training loss generator: 0.734\n",
            "  Average training loss discriminator: 0.806\n",
            "  Training epoch took: 41.13s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 2.051\n",
            "  Test took: 41.24s\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.3s.\n",
            "  Batch 30  of  88.    Elapsed: 13.94s.\n",
            "  Batch 40  of  88.    Elapsed: 18.57s.\n",
            "  Batch 50  of  88.    Elapsed: 23.17s.\n",
            "  Batch 60  of  88.    Elapsed: 27.79s.\n",
            "  Batch 70  of  88.    Elapsed: 32.41s.\n",
            "  Batch 80  of  88.    Elapsed: 37.03s.\n",
            "  Average training loss generator: 0.717\n",
            "  Average training loss discriminator: 0.739\n",
            "  Training epoch took: 40.63s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.539\n",
            "  Test Loss: 2.422\n",
            "  Test took: 40.75s\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.57s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.85s.\n",
            "  Batch 70  of  88.    Elapsed: 32.49s.\n",
            "  Batch 80  of  88.    Elapsed: 37.13s.\n",
            "  Average training loss generator: 0.709\n",
            "  Average training loss discriminator: 0.727\n",
            "  Training epoch took: 40.78s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.539\n",
            "  Test Loss: 2.682\n",
            "  Test took: 40.9s\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.91s.\n",
            "  Batch 40  of  88.    Elapsed: 18.53s.\n",
            "  Batch 50  of  88.    Elapsed: 23.14s.\n",
            "  Batch 60  of  88.    Elapsed: 27.79s.\n",
            "  Batch 70  of  88.    Elapsed: 32.43s.\n",
            "  Batch 80  of  88.    Elapsed: 37.07s.\n",
            "  Average training loss generator: 0.705\n",
            "  Average training loss discriminator: 0.720\n",
            "  Training epoch took: 40.71s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 2.921\n",
            "  Test took: 40.83s\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.59s.\n",
            "  Batch 50  of  88.    Elapsed: 23.24s.\n",
            "  Batch 60  of  88.    Elapsed: 27.88s.\n",
            "  Batch 70  of  88.    Elapsed: 32.5s.\n",
            "  Batch 80  of  88.    Elapsed: 37.14s.\n",
            "  Average training loss generator: 0.702\n",
            "  Average training loss discriminator: 0.716\n",
            "  Training epoch took: 40.77s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 2.926\n",
            "  Test took: 40.89s\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.27s.\n",
            "  Batch 30  of  88.    Elapsed: 13.89s.\n",
            "  Batch 40  of  88.    Elapsed: 18.53s.\n",
            "  Batch 50  of  88.    Elapsed: 23.16s.\n",
            "  Batch 60  of  88.    Elapsed: 27.78s.\n",
            "  Batch 70  of  88.    Elapsed: 32.38s.\n",
            "  Batch 80  of  88.    Elapsed: 37.02s.\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 0.712\n",
            "  Training epoch took: 40.66s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 3.164\n",
            "  Test took: 40.78s\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.28s.\n",
            "  Batch 30  of  88.    Elapsed: 13.89s.\n",
            "  Batch 40  of  88.    Elapsed: 18.52s.\n",
            "  Batch 50  of  88.    Elapsed: 23.17s.\n",
            "  Batch 60  of  88.    Elapsed: 27.82s.\n",
            "  Batch 70  of  88.    Elapsed: 32.42s.\n",
            "  Batch 80  of  88.    Elapsed: 37.08s.\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.710\n",
            "  Training epoch took: 40.71s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.566\n",
            "  Test Loss: 3.293\n",
            "  Test took: 40.83s\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.27s.\n",
            "  Batch 30  of  88.    Elapsed: 13.9s.\n",
            "  Batch 40  of  88.    Elapsed: 18.55s.\n",
            "  Batch 50  of  88.    Elapsed: 23.19s.\n",
            "  Batch 60  of  88.    Elapsed: 27.84s.\n",
            "  Batch 70  of  88.    Elapsed: 32.44s.\n",
            "  Batch 80  of  88.    Elapsed: 37.09s.\n",
            "  Average training loss generator: 0.697\n",
            "  Average training loss discriminator: 0.709\n",
            "  Training epoch took: 40.74s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.553\n",
            "  Test Loss: 3.423\n",
            "  Test took: 40.85s\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.63s.\n",
            "  Batch 20  of  88.    Elapsed: 9.27s.\n",
            "  Batch 30  of  88.    Elapsed: 13.92s.\n",
            "  Batch 40  of  88.    Elapsed: 18.57s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.84s.\n",
            "  Batch 70  of  88.    Elapsed: 32.47s.\n",
            "  Batch 80  of  88.    Elapsed: 37.08s.\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 0.707\n",
            "  Training epoch took: 40.7s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 3.442\n",
            "  Test took: 40.82s\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.56s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.85s.\n",
            "  Batch 70  of  88.    Elapsed: 32.5s.\n",
            "  Batch 80  of  88.    Elapsed: 37.16s.\n",
            "  Average training loss generator: 0.697\n",
            "  Average training loss discriminator: 0.706\n",
            "  Training epoch took: 40.8s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 4.098\n",
            "  Test took: 40.91s\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.28s.\n",
            "  Batch 30  of  88.    Elapsed: 13.92s.\n",
            "  Batch 40  of  88.    Elapsed: 18.56s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.85s.\n",
            "  Batch 70  of  88.    Elapsed: 32.5s.\n",
            "  Batch 80  of  88.    Elapsed: 37.14s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.705\n",
            "  Training epoch took: 40.78s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.474\n",
            "  Test Loss: 4.051\n",
            "  Test took: 40.9s\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.67s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.94s.\n",
            "  Batch 40  of  88.    Elapsed: 18.57s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.84s.\n",
            "  Batch 70  of  88.    Elapsed: 32.46s.\n",
            "  Batch 80  of  88.    Elapsed: 37.1s.\n",
            "  Average training loss generator: 0.729\n",
            "  Average training loss discriminator: 1.955\n",
            "  Training epoch took: 40.73s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.158\n",
            "  Test Loss: 1.571\n",
            "  Test took: 40.85s\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.92s.\n",
            "  Batch 40  of  88.    Elapsed: 18.58s.\n",
            "  Batch 50  of  88.    Elapsed: 23.24s.\n",
            "  Batch 60  of  88.    Elapsed: 27.9s.\n",
            "  Batch 70  of  88.    Elapsed: 32.55s.\n",
            "  Batch 80  of  88.    Elapsed: 37.2s.\n",
            "  Average training loss generator: 0.725\n",
            "  Average training loss discriminator: 1.909\n",
            "  Training epoch took: 40.83s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.355\n",
            "  Test Loss: 1.379\n",
            "  Test took: 40.95s\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.67s.\n",
            "  Batch 20  of  88.    Elapsed: 9.31s.\n",
            "  Batch 30  of  88.    Elapsed: 13.95s.\n",
            "  Batch 40  of  88.    Elapsed: 18.58s.\n",
            "  Batch 50  of  88.    Elapsed: 23.19s.\n",
            "  Batch 60  of  88.    Elapsed: 27.8s.\n",
            "  Batch 70  of  88.    Elapsed: 32.43s.\n",
            "  Batch 80  of  88.    Elapsed: 37.08s.\n",
            "  Average training loss generator: 0.716\n",
            "  Average training loss discriminator: 1.181\n",
            "  Training epoch took: 40.72s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.500\n",
            "  Test Loss: 2.181\n",
            "  Test took: 40.83s\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.62s.\n",
            "  Batch 20  of  88.    Elapsed: 9.24s.\n",
            "  Batch 30  of  88.    Elapsed: 13.89s.\n",
            "  Batch 40  of  88.    Elapsed: 18.52s.\n",
            "  Batch 50  of  88.    Elapsed: 23.15s.\n",
            "  Batch 60  of  88.    Elapsed: 27.78s.\n",
            "  Batch 70  of  88.    Elapsed: 32.41s.\n",
            "  Batch 80  of  88.    Elapsed: 37.04s.\n",
            "  Average training loss generator: 0.707\n",
            "  Average training loss discriminator: 0.804\n",
            "  Training epoch took: 40.66s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 2.710\n",
            "  Test took: 40.78s\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.56s.\n",
            "  Batch 50  of  88.    Elapsed: 23.18s.\n",
            "  Batch 60  of  88.    Elapsed: 27.81s.\n",
            "  Batch 70  of  88.    Elapsed: 32.44s.\n",
            "  Batch 80  of  88.    Elapsed: 37.06s.\n",
            "  Average training loss generator: 0.703\n",
            "  Average training loss discriminator: 0.826\n",
            "  Training epoch took: 40.68s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 2.938\n",
            "  Test took: 40.8s\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.66s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.91s.\n",
            "  Batch 40  of  88.    Elapsed: 18.54s.\n",
            "  Batch 50  of  88.    Elapsed: 23.17s.\n",
            "  Batch 60  of  88.    Elapsed: 27.81s.\n",
            "  Batch 70  of  88.    Elapsed: 32.44s.\n",
            "  Batch 80  of  88.    Elapsed: 37.06s.\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 0.813\n",
            "  Training epoch took: 40.69s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 3.078\n",
            "  Test took: 40.81s\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.57s.\n",
            "  Batch 50  of  88.    Elapsed: 23.19s.\n",
            "  Batch 60  of  88.    Elapsed: 27.82s.\n",
            "  Batch 70  of  88.    Elapsed: 32.44s.\n",
            "  Batch 80  of  88.    Elapsed: 37.06s.\n",
            "  Average training loss generator: 0.701\n",
            "  Average training loss discriminator: 0.790\n",
            "  Training epoch took: 40.7s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.487\n",
            "  Test Loss: 3.402\n",
            "  Test took: 40.82s\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.26s.\n",
            "  Batch 30  of  88.    Elapsed: 13.89s.\n",
            "  Batch 40  of  88.    Elapsed: 18.52s.\n",
            "  Batch 50  of  88.    Elapsed: 23.16s.\n",
            "  Batch 60  of  88.    Elapsed: 27.8s.\n",
            "  Batch 70  of  88.    Elapsed: 32.43s.\n",
            "  Batch 80  of  88.    Elapsed: 37.08s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.787\n",
            "  Training epoch took: 40.72s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 3.405\n",
            "  Test took: 40.84s\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.66s.\n",
            "  Batch 20  of  88.    Elapsed: 9.3s.\n",
            "  Batch 30  of  88.    Elapsed: 13.92s.\n",
            "  Batch 40  of  88.    Elapsed: 18.54s.\n",
            "  Batch 50  of  88.    Elapsed: 23.17s.\n",
            "  Batch 60  of  88.    Elapsed: 27.81s.\n",
            "  Batch 70  of  88.    Elapsed: 32.45s.\n",
            "  Batch 80  of  88.    Elapsed: 37.09s.\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 0.800\n",
            "  Training epoch took: 40.71s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.500\n",
            "  Test Loss: 3.711\n",
            "  Test took: 40.83s\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.56s.\n",
            "  Batch 50  of  88.    Elapsed: 23.21s.\n",
            "  Batch 60  of  88.    Elapsed: 27.86s.\n",
            "  Batch 70  of  88.    Elapsed: 32.5s.\n",
            "  Batch 80  of  88.    Elapsed: 37.14s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.786\n",
            "  Training epoch took: 40.77s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 3.711\n",
            "  Test took: 40.89s\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.64s.\n",
            "  Batch 20  of  88.    Elapsed: 9.28s.\n",
            "  Batch 30  of  88.    Elapsed: 13.9s.\n",
            "  Batch 40  of  88.    Elapsed: 18.54s.\n",
            "  Batch 50  of  88.    Elapsed: 23.18s.\n",
            "  Batch 60  of  88.    Elapsed: 27.81s.\n",
            "  Batch 70  of  88.    Elapsed: 32.45s.\n",
            "  Batch 80  of  88.    Elapsed: 37.06s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.801\n",
            "  Training epoch took: 40.68s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 3.712\n",
            "  Test took: 40.8s\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.27s.\n",
            "  Batch 30  of  88.    Elapsed: 13.89s.\n",
            "  Batch 40  of  88.    Elapsed: 18.5s.\n",
            "  Batch 50  of  88.    Elapsed: 23.13s.\n",
            "  Batch 60  of  88.    Elapsed: 27.76s.\n",
            "  Batch 70  of  88.    Elapsed: 32.38s.\n",
            "  Batch 80  of  88.    Elapsed: 37.0s.\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.781\n",
            "  Training epoch took: 40.63s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.513\n",
            "  Test Loss: 3.838\n",
            "  Test took: 40.75s\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.66s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.58s.\n",
            "  Batch 50  of  88.    Elapsed: 23.21s.\n",
            "  Batch 60  of  88.    Elapsed: 27.86s.\n",
            "  Batch 70  of  88.    Elapsed: 32.47s.\n",
            "  Batch 80  of  88.    Elapsed: 37.11s.\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.812\n",
            "  Training epoch took: 40.76s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 4.183\n",
            "  Test took: 40.87s\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.66s.\n",
            "  Batch 20  of  88.    Elapsed: 9.3s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.56s.\n",
            "  Batch 50  of  88.    Elapsed: 23.2s.\n",
            "  Batch 60  of  88.    Elapsed: 27.82s.\n",
            "  Batch 70  of  88.    Elapsed: 32.46s.\n",
            "  Batch 80  of  88.    Elapsed: 37.09s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.798\n",
            "  Training epoch took: 40.72s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.539\n",
            "  Test Loss: 4.153\n",
            "  Test took: 40.84s\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.65s.\n",
            "  Batch 20  of  88.    Elapsed: 9.29s.\n",
            "  Batch 30  of  88.    Elapsed: 13.93s.\n",
            "  Batch 40  of  88.    Elapsed: 18.58s.\n",
            "  Batch 50  of  88.    Elapsed: 23.21s.\n",
            "  Batch 60  of  88.    Elapsed: 27.87s.\n",
            "  Batch 70  of  88.    Elapsed: 32.53s.\n",
            "  Batch 80  of  88.    Elapsed: 37.18s.\n",
            "  Average training loss generator: 0.697\n",
            "  Average training loss discriminator: 0.794\n",
            "  Training epoch took: 40.81s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 4.112\n",
            "  Test took: 40.93s\n",
            "\n",
            "======== Epoch 29 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.63s.\n",
            "  Batch 20  of  88.    Elapsed: 9.26s.\n",
            "  Batch 30  of  88.    Elapsed: 13.88s.\n",
            "  Batch 40  of  88.    Elapsed: 18.5s.\n",
            "  Batch 50  of  88.    Elapsed: 23.12s.\n",
            "  Batch 60  of  88.    Elapsed: 27.74s.\n",
            "  Batch 70  of  88.    Elapsed: 32.36s.\n",
            "  Batch 80  of  88.    Elapsed: 36.97s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.792\n",
            "  Training epoch took: 40.58s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 4.172\n",
            "  Test took: 40.7s\n",
            "\n",
            "======== Epoch 30 / 30 ========\n",
            "Training...\n",
            "  Batch 10  of  88.    Elapsed: 4.61s.\n",
            "  Batch 20  of  88.    Elapsed: 9.21s.\n",
            "  Batch 30  of  88.    Elapsed: 13.83s.\n",
            "  Batch 40  of  88.    Elapsed: 18.45s.\n",
            "  Batch 50  of  88.    Elapsed: 23.09s.\n",
            "  Batch 60  of  88.    Elapsed: 27.73s.\n",
            "  Batch 70  of  88.    Elapsed: 32.38s.\n",
            "  Batch 80  of  88.    Elapsed: 37.02s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.794\n",
            "  Training epoch took: 40.67s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.526\n",
            "  Test Loss: 4.144\n",
            "  Test took: 40.78s\n",
            "\n",
            "Final Evaluation...\n",
            "Final Recall: 0.526\n",
            "Final Precision: 0.624\n",
            "Final F1 Score: 0.521\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Função para calcular Recall, Precision e F1-Score manualmente para múltiplas classes\n",
        "def calculate_recall_precision_f1_multiclass(y_true, y_pred):\n",
        "    # Classes únicas\n",
        "    classes = np.unique(y_true)\n",
        "\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for cls in classes:\n",
        "        # True Positives, False Positives, False Negatives para a classe atual\n",
        "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
        "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
        "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
        "\n",
        "        # Recall\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # Precision\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        precisions.append(precision)\n",
        "\n",
        "        # F1 Score\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Média das métricas por classe\n",
        "    avg_recall = np.mean(recalls)\n",
        "    avg_precision = np.mean(precisions)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    return avg_recall, avg_precision, avg_f1\n",
        "\n",
        "# Função auxiliar para formatar o tempo\n",
        "def format_time(elapsed):\n",
        "    return str(round(elapsed, 2)) + \"s\"\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# Optimizers\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)\n",
        "\n",
        "# Scheduler\n",
        "if apply_scheduler:\n",
        "    num_train_examples = len(train_examples)\n",
        "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps=num_warmup_steps)\n",
        "    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Treinamento do modelo\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    print(\"\")\n",
        "    print(f\"======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n",
        "    print(\"Training...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    transformer.train()\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f\"  Batch {step:,}  of  {len(train_dataloader):,}.    Elapsed: {elapsed}.\")\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "\n",
        "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features, D_fake_features = features_list\n",
        "\n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits, D_fake_logits = logits_list\n",
        "\n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs, D_fake_probs = probs_list\n",
        "\n",
        "        # Generator loss\n",
        "        g_loss_d = -torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "        # Discriminator loss\n",
        "        logits = D_real_logits[:, :-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        label2one_hot = F.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.numel()\n",
        "\n",
        "        if labeled_example_count == 0:\n",
        "            D_L_Supervised = 0\n",
        "        else:\n",
        "            D_L_Supervised = torch.sum(per_example_loss) / labeled_example_count\n",
        "\n",
        "        D_L_unsupervised1U = -torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward()\n",
        "\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        if apply_scheduler:\n",
        "            scheduler_d.step()\n",
        "            scheduler_g.step()\n",
        "\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"  Average training loss generator: {avg_train_loss_g:.3f}\")\n",
        "    print(f\"  Average training loss discriminator: {avg_train_loss_d:.3f}\")\n",
        "    print(f\"  Training epoch took: {training_time}\")\n",
        "\n",
        "    # Avaliação por época para calcular a acurácia\n",
        "    print(\"\\nRunning Test...\")\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "    total_test_loss = 0\n",
        "\n",
        "    transformer.eval()\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            filtered_logits = logits[:, :-1]\n",
        "            total_test_loss += F.cross_entropy(filtered_logits, b_labels, ignore_index=-1)\n",
        "\n",
        "            _, preds = torch.max(filtered_logits, 1)\n",
        "            all_preds += preds.detach().cpu()\n",
        "            all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Cálculo da acurácia para cada rodada\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "\n",
        "    print(f\"  Accuracy: {test_accuracy:.3f}\")\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    test_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"  Test Loss: {avg_test_loss:.3f}\")\n",
        "    print(f\"  Test took: {test_time}\")\n",
        "\n",
        "    training_stats.append({\n",
        "        'epoch': epoch_i + 1,\n",
        "        'Training Loss generator': avg_train_loss_g,\n",
        "        'Training Loss discriminator': avg_train_loss_d,\n",
        "        'Valid. Loss': avg_test_loss,\n",
        "        'Valid. Accur.': test_accuracy,\n",
        "        'Training Time': training_time,\n",
        "        'Test Time': test_time\n",
        "    })\n",
        "\n",
        "# Avaliação do modelo no final do treinamento\n",
        "print(\"\\nFinal Evaluation...\")\n",
        "\n",
        "recall, precision, f1 = calculate_recall_precision_f1_multiclass(all_labels_ids, all_preds)\n",
        "\n",
        "print(f\"Final Recall: {recall:.3f}\")\n",
        "print(f\"Final Precision: {precision:.3f}\")\n",
        "print(f\"Final F1 Score: {f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDm9NProRB4c",
        "outputId": "09ec4eeb-7d5e-4207-ef8e-8dac9083568e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'Training Loss generator': 0.3116018914363601, 'Training Loss discriminator': 3.174262296069752, 'Valid. Loss': 1.3811149597167969, 'Valid. Accur.': 0.3684210526315789, 'Training Time': '41.5s', 'Test Time': '41.63s'}\n",
            "{'epoch': 2, 'Training Loss generator': 0.6958502900194038, 'Training Loss discriminator': 1.5277129845185713, 'Valid. Loss': 2.0696654319763184, 'Valid. Accur.': 0.5, 'Training Time': '40.41s', 'Test Time': '40.53s'}\n",
            "{'epoch': 3, 'Training Loss generator': 0.7336847626350143, 'Training Loss discriminator': 0.8056033964861523, 'Valid. Loss': 2.0506820678710938, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '41.13s', 'Test Time': '41.24s'}\n",
            "{'epoch': 4, 'Training Loss generator': 0.7165496240962635, 'Training Loss discriminator': 0.7387987266887318, 'Valid. Loss': 2.42191219329834, 'Valid. Accur.': 0.5394736842105263, 'Training Time': '40.63s', 'Test Time': '40.75s'}\n",
            "{'epoch': 5, 'Training Loss generator': 0.7086100449616258, 'Training Loss discriminator': 0.7267979911782525, 'Valid. Loss': 2.6816904544830322, 'Valid. Accur.': 0.5394736842105263, 'Training Time': '40.78s', 'Test Time': '40.9s'}\n",
            "{'epoch': 6, 'Training Loss generator': 0.7052680971947584, 'Training Loss discriminator': 0.7198792296377096, 'Valid. Loss': 2.921387195587158, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.71s', 'Test Time': '40.83s'}\n",
            "{'epoch': 7, 'Training Loss generator': 0.7018298371271654, 'Training Loss discriminator': 0.7161006202751939, 'Valid. Loss': 2.926208019256592, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.77s', 'Test Time': '40.89s'}\n",
            "{'epoch': 8, 'Training Loss generator': 0.6997071741656824, 'Training Loss discriminator': 0.7123570015484636, 'Valid. Loss': 3.164309024810791, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.66s', 'Test Time': '40.78s'}\n",
            "{'epoch': 9, 'Training Loss generator': 0.6991973167115991, 'Training Loss discriminator': 0.7099565226923336, 'Valid. Loss': 3.2934179306030273, 'Valid. Accur.': 0.5657894736842105, 'Training Time': '40.71s', 'Test Time': '40.83s'}\n",
            "{'epoch': 10, 'Training Loss generator': 0.6972370926629413, 'Training Loss discriminator': 0.7086508213119074, 'Valid. Loss': 3.4225125312805176, 'Valid. Accur.': 0.5526315789473685, 'Training Time': '40.74s', 'Test Time': '40.85s'}\n",
            "{'epoch': 11, 'Training Loss generator': 0.6997908482497389, 'Training Loss discriminator': 0.7066946009343321, 'Valid. Loss': 3.4415667057037354, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.7s', 'Test Time': '40.82s'}\n",
            "{'epoch': 12, 'Training Loss generator': 0.6973580352284692, 'Training Loss discriminator': 0.7058803669430993, 'Valid. Loss': 4.097837924957275, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.8s', 'Test Time': '40.91s'}\n",
            "{'epoch': 13, 'Training Loss generator': 0.6981215829199011, 'Training Loss discriminator': 0.7048822776837782, 'Valid. Loss': 4.051175594329834, 'Valid. Accur.': 0.47368421052631576, 'Training Time': '40.78s', 'Test Time': '40.9s'}\n",
            "{'epoch': 14, 'Training Loss generator': 0.7287775217132135, 'Training Loss discriminator': 1.9554251784628087, 'Valid. Loss': 1.571052074432373, 'Valid. Accur.': 0.15789473684210525, 'Training Time': '40.73s', 'Test Time': '40.85s'}\n",
            "{'epoch': 15, 'Training Loss generator': 0.7249022945761681, 'Training Loss discriminator': 1.9087434058839625, 'Valid. Loss': 1.379063367843628, 'Valid. Accur.': 0.35526315789473684, 'Training Time': '40.83s', 'Test Time': '40.95s'}\n",
            "{'epoch': 16, 'Training Loss generator': 0.7159954681992531, 'Training Loss discriminator': 1.1812348108399997, 'Valid. Loss': 2.1810808181762695, 'Valid. Accur.': 0.5, 'Training Time': '40.72s', 'Test Time': '40.83s'}\n",
            "{'epoch': 17, 'Training Loss generator': 0.7067239745096727, 'Training Loss discriminator': 0.8035107735883106, 'Valid. Loss': 2.7098076343536377, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.66s', 'Test Time': '40.78s'}\n",
            "{'epoch': 18, 'Training Loss generator': 0.7027407092126933, 'Training Loss discriminator': 0.8258795643394644, 'Valid. Loss': 2.9379286766052246, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.68s', 'Test Time': '40.8s'}\n",
            "{'epoch': 19, 'Training Loss generator': 0.7003473050215028, 'Training Loss discriminator': 0.8134249049154195, 'Valid. Loss': 3.0783538818359375, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.69s', 'Test Time': '40.81s'}\n",
            "{'epoch': 20, 'Training Loss generator': 0.7006179141727361, 'Training Loss discriminator': 0.7900333939628168, 'Valid. Loss': 3.4016897678375244, 'Valid. Accur.': 0.4868421052631579, 'Training Time': '40.7s', 'Test Time': '40.82s'}\n",
            "{'epoch': 21, 'Training Loss generator': 0.6984706317836588, 'Training Loss discriminator': 0.7872347642074932, 'Valid. Loss': 3.4048714637756348, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.72s', 'Test Time': '40.84s'}\n",
            "{'epoch': 22, 'Training Loss generator': 0.7003950144756924, 'Training Loss discriminator': 0.7996129407124086, 'Valid. Loss': 3.7107675075531006, 'Valid. Accur.': 0.5, 'Training Time': '40.71s', 'Test Time': '40.83s'}\n",
            "{'epoch': 23, 'Training Loss generator': 0.6980222131718289, 'Training Loss discriminator': 0.7855437330224297, 'Valid. Loss': 3.7112433910369873, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.77s', 'Test Time': '40.89s'}\n",
            "{'epoch': 24, 'Training Loss generator': 0.698004396801645, 'Training Loss discriminator': 0.8014930520545352, 'Valid. Loss': 3.712411880493164, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.68s', 'Test Time': '40.8s'}\n",
            "{'epoch': 25, 'Training Loss generator': 0.6988075592301108, 'Training Loss discriminator': 0.781077734448693, 'Valid. Loss': 3.8383586406707764, 'Valid. Accur.': 0.5131578947368421, 'Training Time': '40.63s', 'Test Time': '40.75s'}\n",
            "{'epoch': 26, 'Training Loss generator': 0.6985027546232397, 'Training Loss discriminator': 0.8123373782092874, 'Valid. Loss': 4.183095932006836, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.76s', 'Test Time': '40.87s'}\n",
            "{'epoch': 27, 'Training Loss generator': 0.6980611085891724, 'Training Loss discriminator': 0.7980101826516065, 'Valid. Loss': 4.152716636657715, 'Valid. Accur.': 0.5394736842105263, 'Training Time': '40.72s', 'Test Time': '40.84s'}\n",
            "{'epoch': 28, 'Training Loss generator': 0.6970102353529497, 'Training Loss discriminator': 0.7943557480519469, 'Valid. Loss': 4.111634254455566, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.81s', 'Test Time': '40.93s'}\n",
            "{'epoch': 29, 'Training Loss generator': 0.698245641860095, 'Training Loss discriminator': 0.792279189960523, 'Valid. Loss': 4.172116279602051, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.58s', 'Test Time': '40.7s'}\n",
            "{'epoch': 30, 'Training Loss generator': 0.6981850212270563, 'Training Loss discriminator': 0.793806141750379, 'Valid. Loss': 4.144248962402344, 'Valid. Accur.': 0.5263157894736842, 'Training Time': '40.67s', 'Test Time': '40.78s'}\n",
            "\n",
            "Training complete!\n",
            "Total training took 1225.96s (h:mm:ss)\n",
            "Tempo de execução: 1240.6117 segundos\n"
          ]
        }
      ],
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "resultado = sum(range(10**6))\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Tempo de execução: {execution_time:.4f} segundos\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
