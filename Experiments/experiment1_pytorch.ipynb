{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT (in Pytorch and compatible with HuggingFace)\n",
        "\n",
        "This is a Pytorch (+ **Huggingface** transformers) implementation of the GAN-BERT model from https://github.com/crux82/ganbert. While the original GAN-BERT was an extension of BERT, this implementation can be adapted to several architectures, ranging from Roberta to Albert!\n",
        "\n",
        "**NOTE**: given that this implementation is different from the original one in Tensorflow, some results can be slighty different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0m5KR34gmRH"
      },
      "source": [
        "Let's GO!\n",
        "\n",
        "Required Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "645040f1-0c4d-426e-9d84-73574fec4a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.3.2\n",
            "  Using cached transformers-4.3.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.3.2)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.3.2)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2024.12.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (1.4.2)\n",
            "Using cached transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "213e2085-6c81-4e16-a710-a65007fa2db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw0HC_hU3FUy",
        "outputId": "c8c58c1c-f0f7-45f0-a945-08899ede1260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ganbert' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator,\n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1;\n",
        "# number of hidden layers in the discriminator,\n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1;\n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.1\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets,\n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 10\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\" TESTAR\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "#model_name = \"amazon/bort\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "! git clone https://github.com/mauriciokonrath/ganbert.git\n",
        "\n",
        "#  NOTE: in this setting 50 classes are involved\n",
        "labeled_file = \"./ganbert/data/standardized_test_withoutSub.tsv\"\n",
        "unlabeled_file = \"./ganbert/data/unlabeled.tsv\"\n",
        "test_filename = \"./ganbert/data/standardized_labeled_withoutSub.tsv\"\n",
        "\n",
        "\n",
        "label_list = [\"UNK_UNK\",\"NUM_num\", \"DESC_desc\", \"HUM_hum\",\n",
        "              \"ENTY_enty\", \"LOC_loc\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxghkkZq3Gbn",
        "outputId": "788dad18-3f87-4d1a-db90-49c92d59ed56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W7cP8q7K3BId"
      },
      "outputs": [],
      "source": [
        "def get_qc_examples(input_file):\n",
        "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "  examples = []\n",
        "\n",
        "  with open(input_file, 'r') as f:\n",
        "      contents = f.read()\n",
        "      file_as_list = contents.splitlines()\n",
        "      for line in file_as_list[1:]:\n",
        "          split = line.split(\" \")\n",
        "          question = ' '.join(split[1:])\n",
        "\n",
        "          text_a = question\n",
        "          inn_split = split[0].split(\":\")\n",
        "          label = inn_split[0] + \"_\" + inn_split[1]\n",
        "          examples.append((text_a, label))\n",
        "      f.close()\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43tOavNqib4"
      },
      "source": [
        "**Load** the input QC dataset (fine-grained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cXCwFyF2qhw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Load the examples\n",
        "labeled_examples = get_qc_examples(labeled_file)\n",
        "unlabeled_examples = get_qc_examples(unlabeled_file)\n",
        "test_examples = get_qc_examples(test_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "outputs": [],
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are\n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples\n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask:\n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples):\n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "\n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization\n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "\n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset),\n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c-nsMXlKX-D",
        "outputId": "a7d862f5-a4d4-4131-8c1a-09cc8a46bd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-16c38d5f7491>:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  label_mask_array = torch.tensor(label_mask_array)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = labeled_examples\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "#If unlabel examples are available\n",
        "if unlabeled_examples:\n",
        "  train_examples = train_examples + unlabeled_examples\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "#------------------------------\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ihcw3vquaQm"
      },
      "source": [
        "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "outputs": [],
      "source": [
        "\n",
        "#------------------------------\n",
        "#   The Generator as in\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uje9s2zQunFc"
      },
      "source": [
        "We instantiate the Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylz5rvqE3U2S",
        "outputId": "f4f8d166-e0d7-49a3-d87f-7100754fcb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The config file is required to get the dimension of the vector produced by\n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "# print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3qzp2-usZE"
      },
      "source": [
        "Let's go with the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhqylHGK3Va4",
        "outputId": "0b9f04d9-5f6a-4acb-f74c-b8a43b911c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.29s.\n",
            "  Batch 20  of  104.    Elapsed: 18.67s.\n",
            "  Batch 30  of  104.    Elapsed: 28.2s.\n",
            "  Batch 40  of  104.    Elapsed: 37.83s.\n",
            "  Batch 50  of  104.    Elapsed: 47.35s.\n",
            "  Batch 60  of  104.    Elapsed: 56.74s.\n",
            "  Batch 70  of  104.    Elapsed: 66.04s.\n",
            "  Batch 80  of  104.    Elapsed: 75.28s.\n",
            "  Batch 90  of  104.    Elapsed: 84.5s.\n",
            "  Batch 100  of  104.    Elapsed: 93.75s.\n",
            "  Average training loss generator: 0.670\n",
            "  Average training loss discriminator: 1.650\n",
            "  Training epoch took: 97.29s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.860\n",
            "  Test Loss: 0.582\n",
            "  Test took: 97.46s\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.33s.\n",
            "  Batch 20  of  104.    Elapsed: 18.71s.\n",
            "  Batch 30  of  104.    Elapsed: 28.11s.\n",
            "  Batch 40  of  104.    Elapsed: 37.51s.\n",
            "  Batch 50  of  104.    Elapsed: 46.87s.\n",
            "  Batch 60  of  104.    Elapsed: 56.2s.\n",
            "  Batch 70  of  104.    Elapsed: 65.51s.\n",
            "  Batch 80  of  104.    Elapsed: 74.82s.\n",
            "  Batch 90  of  104.    Elapsed: 84.12s.\n",
            "  Batch 100  of  104.    Elapsed: 93.44s.\n",
            "  Average training loss generator: 0.726\n",
            "  Average training loss discriminator: 0.796\n",
            "  Training epoch took: 97.0s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.417\n",
            "  Test took: 97.16s\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.35s.\n",
            "  Batch 20  of  104.    Elapsed: 18.7s.\n",
            "  Batch 30  of  104.    Elapsed: 28.04s.\n",
            "  Batch 40  of  104.    Elapsed: 37.4s.\n",
            "  Batch 50  of  104.    Elapsed: 46.76s.\n",
            "  Batch 60  of  104.    Elapsed: 56.11s.\n",
            "  Batch 70  of  104.    Elapsed: 65.45s.\n",
            "  Batch 80  of  104.    Elapsed: 74.76s.\n",
            "  Batch 90  of  104.    Elapsed: 84.06s.\n",
            "  Batch 100  of  104.    Elapsed: 93.35s.\n",
            "  Average training loss generator: 0.708\n",
            "  Average training loss discriminator: 0.722\n",
            "  Training epoch took: 96.9s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.364\n",
            "  Test took: 97.06s\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.33s.\n",
            "  Batch 20  of  104.    Elapsed: 18.68s.\n",
            "  Batch 30  of  104.    Elapsed: 28.04s.\n",
            "  Batch 40  of  104.    Elapsed: 37.42s.\n",
            "  Batch 50  of  104.    Elapsed: 46.79s.\n",
            "  Batch 60  of  104.    Elapsed: 56.14s.\n",
            "  Batch 70  of  104.    Elapsed: 65.46s.\n",
            "  Batch 80  of  104.    Elapsed: 74.76s.\n",
            "  Batch 90  of  104.    Elapsed: 84.07s.\n",
            "  Batch 100  of  104.    Elapsed: 93.39s.\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 0.713\n",
            "  Training epoch took: 96.95s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.430\n",
            "  Test took: 97.11s\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.35s.\n",
            "  Batch 20  of  104.    Elapsed: 18.71s.\n",
            "  Batch 30  of  104.    Elapsed: 28.08s.\n",
            "  Batch 40  of  104.    Elapsed: 37.45s.\n",
            "  Batch 50  of  104.    Elapsed: 46.79s.\n",
            "  Batch 60  of  104.    Elapsed: 56.12s.\n",
            "  Batch 70  of  104.    Elapsed: 65.44s.\n",
            "  Batch 80  of  104.    Elapsed: 74.74s.\n",
            "  Batch 90  of  104.    Elapsed: 84.06s.\n",
            "  Batch 100  of  104.    Elapsed: 93.36s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.709\n",
            "  Training epoch took: 96.92s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.449\n",
            "  Test took: 97.09s\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.34s.\n",
            "  Batch 20  of  104.    Elapsed: 18.65s.\n",
            "  Batch 30  of  104.    Elapsed: 27.99s.\n",
            "  Batch 40  of  104.    Elapsed: 37.32s.\n",
            "  Batch 50  of  104.    Elapsed: 46.66s.\n",
            "  Batch 60  of  104.    Elapsed: 56.02s.\n",
            "  Batch 70  of  104.    Elapsed: 65.38s.\n",
            "  Batch 80  of  104.    Elapsed: 74.73s.\n",
            "  Batch 90  of  104.    Elapsed: 84.08s.\n",
            "  Batch 100  of  104.    Elapsed: 93.44s.\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.706\n",
            "  Training epoch took: 97.01s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.450\n",
            "  Test took: 97.18s\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.36s.\n",
            "  Batch 20  of  104.    Elapsed: 18.7s.\n",
            "  Batch 30  of  104.    Elapsed: 28.04s.\n",
            "  Batch 40  of  104.    Elapsed: 37.39s.\n",
            "  Batch 50  of  104.    Elapsed: 46.72s.\n",
            "  Batch 60  of  104.    Elapsed: 56.06s.\n",
            "  Batch 70  of  104.    Elapsed: 65.39s.\n",
            "  Batch 80  of  104.    Elapsed: 74.7s.\n",
            "  Batch 90  of  104.    Elapsed: 84.01s.\n",
            "  Batch 100  of  104.    Elapsed: 93.35s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.704\n",
            "  Training epoch took: 96.92s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.940\n",
            "  Test Loss: 0.467\n",
            "  Test took: 97.08s\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.39s.\n",
            "  Batch 20  of  104.    Elapsed: 18.76s.\n",
            "  Batch 30  of  104.    Elapsed: 28.11s.\n",
            "  Batch 40  of  104.    Elapsed: 37.44s.\n",
            "  Batch 50  of  104.    Elapsed: 46.74s.\n",
            "  Batch 60  of  104.    Elapsed: 56.06s.\n",
            "  Batch 70  of  104.    Elapsed: 65.39s.\n",
            "  Batch 80  of  104.    Elapsed: 74.71s.\n",
            "  Batch 90  of  104.    Elapsed: 84.06s.\n",
            "  Batch 100  of  104.    Elapsed: 93.4s.\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.703\n",
            "  Training epoch took: 96.96s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.940\n",
            "  Test Loss: 0.499\n",
            "  Test took: 97.13s\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.34s.\n",
            "  Batch 20  of  104.    Elapsed: 18.65s.\n",
            "  Batch 30  of  104.    Elapsed: 27.97s.\n",
            "  Batch 40  of  104.    Elapsed: 37.31s.\n",
            "  Batch 50  of  104.    Elapsed: 46.66s.\n",
            "  Batch 60  of  104.    Elapsed: 56.03s.\n",
            "  Batch 70  of  104.    Elapsed: 65.38s.\n",
            "  Batch 80  of  104.    Elapsed: 74.75s.\n",
            "  Batch 90  of  104.    Elapsed: 84.11s.\n",
            "  Batch 100  of  104.    Elapsed: 93.47s.\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.702\n",
            "  Training epoch took: 97.03s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.646\n",
            "  Test took: 97.2s\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch 10  of  104.    Elapsed: 9.36s.\n",
            "  Batch 20  of  104.    Elapsed: 18.72s.\n",
            "  Batch 30  of  104.    Elapsed: 28.06s.\n",
            "  Batch 40  of  104.    Elapsed: 37.38s.\n",
            "  Batch 50  of  104.    Elapsed: 46.72s.\n",
            "  Batch 60  of  104.    Elapsed: 56.04s.\n",
            "  Batch 70  of  104.    Elapsed: 65.37s.\n",
            "  Batch 80  of  104.    Elapsed: 74.7s.\n",
            "  Batch 90  of  104.    Elapsed: 84.03s.\n",
            "  Batch 100  of  104.    Elapsed: 93.36s.\n",
            "  Average training loss generator: 0.697\n",
            "  Average training loss discriminator: 0.702\n",
            "  Training epoch took: 96.93s\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.940\n",
            "  Test Loss: 0.539\n",
            "  Test took: 97.09s\n",
            "\n",
            "Final Evaluation...\n",
            "Final Recall: 0.940\n",
            "Final Precision: 0.954\n",
            "Final F1 Score: 0.939\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Função para calcular Recall, Precision e F1-Score manualmente para múltiplas classes\n",
        "def calculate_recall_precision_f1_multiclass(y_true, y_pred):\n",
        "    # Classes únicas\n",
        "    classes = np.unique(y_true)\n",
        "\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for cls in classes:\n",
        "        # True Positives, False Positives, False Negatives para a classe atual\n",
        "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
        "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
        "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
        "\n",
        "        # Recall\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # Precision\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        precisions.append(precision)\n",
        "\n",
        "        # F1 Score\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Média das métricas por classe\n",
        "    avg_recall = np.mean(recalls)\n",
        "    avg_precision = np.mean(precisions)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    return avg_recall, avg_precision, avg_f1\n",
        "\n",
        "# Função auxiliar para formatar o tempo\n",
        "def format_time(elapsed):\n",
        "    return str(round(elapsed, 2)) + \"s\"\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# Optimizers\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)\n",
        "\n",
        "# Scheduler\n",
        "if apply_scheduler:\n",
        "    num_train_examples = len(train_examples)\n",
        "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, num_warmup_steps=num_warmup_steps)\n",
        "    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Treinamento do modelo\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    print(\"\")\n",
        "    print(f\"======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n",
        "    print(\"Training...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    transformer.train()\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f\"  Batch {step:,}  of  {len(train_dataloader):,}.    Elapsed: {elapsed}.\")\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "\n",
        "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features, D_fake_features = features_list\n",
        "\n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits, D_fake_logits = logits_list\n",
        "\n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs, D_fake_probs = probs_list\n",
        "\n",
        "        # Generator loss\n",
        "        g_loss_d = -torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "        # Discriminator loss\n",
        "        logits = D_real_logits[:, :-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        label2one_hot = F.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.numel()\n",
        "\n",
        "        if labeled_example_count == 0:\n",
        "            D_L_Supervised = 0\n",
        "        else:\n",
        "            D_L_Supervised = torch.sum(per_example_loss) / labeled_example_count\n",
        "\n",
        "        D_L_unsupervised1U = -torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward()\n",
        "\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        if apply_scheduler:\n",
        "            scheduler_d.step()\n",
        "            scheduler_g.step()\n",
        "\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"  Average training loss generator: {avg_train_loss_g:.3f}\")\n",
        "    print(f\"  Average training loss discriminator: {avg_train_loss_d:.3f}\")\n",
        "    print(f\"  Training epoch took: {training_time}\")\n",
        "\n",
        "    # Avaliação por época para calcular a acurácia\n",
        "    print(\"\\nRunning Test...\")\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "    total_test_loss = 0\n",
        "\n",
        "    transformer.eval()\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            filtered_logits = logits[:, :-1]\n",
        "            total_test_loss += F.cross_entropy(filtered_logits, b_labels, ignore_index=-1)\n",
        "\n",
        "            _, preds = torch.max(filtered_logits, 1)\n",
        "            all_preds += preds.detach().cpu()\n",
        "            all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Cálculo da acurácia para cada rodada\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "\n",
        "    print(f\"  Accuracy: {test_accuracy:.3f}\")\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    test_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"  Test Loss: {avg_test_loss:.3f}\")\n",
        "    print(f\"  Test took: {test_time}\")\n",
        "\n",
        "    training_stats.append({\n",
        "        'epoch': epoch_i + 1,\n",
        "        'Training Loss generator': avg_train_loss_g,\n",
        "        'Training Loss discriminator': avg_train_loss_d,\n",
        "        'Valid. Loss': avg_test_loss,\n",
        "        'Valid. Accur.': test_accuracy,\n",
        "        'Training Time': training_time,\n",
        "        'Test Time': test_time\n",
        "    })\n",
        "\n",
        "# Avaliação do modelo no final do treinamento\n",
        "print(\"\\nFinal Evaluation...\")\n",
        "\n",
        "recall, precision, f1 = calculate_recall_precision_f1_multiclass(all_labels_ids, all_preds)\n",
        "\n",
        "print(f\"Final Recall: {recall:.3f}\")\n",
        "print(f\"Final Precision: {precision:.3f}\")\n",
        "print(f\"Final F1 Score: {f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dDm9NProRB4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98369575-2d16-40fa-be97-76caf8358fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 1, 'Training Loss generator': 0.6696677081860029, 'Training Loss discriminator': 1.6498607110518675, 'Valid. Loss': 0.5822218060493469, 'Valid. Accur.': 0.86, 'Training Time': '97.29s', 'Test Time': '97.46s'}\n",
            "{'epoch': 2, 'Training Loss generator': 0.726093362730283, 'Training Loss discriminator': 0.7960128927460084, 'Valid. Loss': 0.4173244833946228, 'Valid. Accur.': 0.92, 'Training Time': '97.0s', 'Test Time': '97.16s'}\n",
            "{'epoch': 3, 'Training Loss generator': 0.707857839190043, 'Training Loss discriminator': 0.7223200528667524, 'Valid. Loss': 0.3639635443687439, 'Valid. Accur.': 0.92, 'Training Time': '96.9s', 'Test Time': '97.06s'}\n",
            "{'epoch': 4, 'Training Loss generator': 0.7001083252521662, 'Training Loss discriminator': 0.7128822494011658, 'Valid. Loss': 0.4301259219646454, 'Valid. Accur.': 0.92, 'Training Time': '96.95s', 'Test Time': '97.11s'}\n",
            "{'epoch': 5, 'Training Loss generator': 0.698460272871531, 'Training Loss discriminator': 0.708876904959862, 'Valid. Loss': 0.4485035836696625, 'Valid. Accur.': 0.92, 'Training Time': '96.92s', 'Test Time': '97.09s'}\n",
            "{'epoch': 6, 'Training Loss generator': 0.6985606459470896, 'Training Loss discriminator': 0.706214017019822, 'Valid. Loss': 0.4498724341392517, 'Valid. Accur.': 0.92, 'Training Time': '97.01s', 'Test Time': '97.18s'}\n",
            "{'epoch': 7, 'Training Loss generator': 0.6977658191552529, 'Training Loss discriminator': 0.7043755874037743, 'Valid. Loss': 0.46677762269973755, 'Valid. Accur.': 0.94, 'Training Time': '96.92s', 'Test Time': '97.08s'}\n",
            "{'epoch': 8, 'Training Loss generator': 0.697500980244233, 'Training Loss discriminator': 0.7032657787203789, 'Valid. Loss': 0.4988817572593689, 'Valid. Accur.': 0.94, 'Training Time': '96.96s', 'Test Time': '97.13s'}\n",
            "{'epoch': 9, 'Training Loss generator': 0.6987559033127931, 'Training Loss discriminator': 0.7021583605271119, 'Valid. Loss': 0.645506739616394, 'Valid. Accur.': 0.92, 'Training Time': '97.03s', 'Test Time': '97.2s'}\n",
            "{'epoch': 10, 'Training Loss generator': 0.6974857736092347, 'Training Loss discriminator': 0.7016556199926597, 'Valid. Loss': 0.5392810106277466, 'Valid. Accur.': 0.94, 'Training Time': '96.93s', 'Test Time': '97.09s'}\n",
            "\n",
            "Training complete!\n",
            "Total training took 971.58s (h:mm:ss)\n",
            "Tempo de execução: 980.6746 segundos\n"
          ]
        }
      ],
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "resultado = sum(range(10**6))\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Tempo de execução: {execution_time:.4f} segundos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fMtWM1SGr-kv"
      },
      "outputs": [],
      "source": [
        "#ADICIONADO\n",
        "# Salvar os modelos\n",
        "torch.save(generator.state_dict(), 'generator.pt')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pt')\n",
        "torch.save(transformer.state_dict(), 'transformer.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8du4uiBMr_LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2834220f-3009-4688-c0b4-242d4833d4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-a3c231b017ae>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  generator.load_state_dict(torch.load('generator.pt'))\n",
            "<ipython-input-25-a3c231b017ae>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  discriminator.load_state_dict(torch.load('discriminator.pt'))\n",
            "<ipython-input-25-a3c231b017ae>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  transformer.load_state_dict(torch.load('transformer.pt'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Carregar os modelos\n",
        "generator.load_state_dict(torch.load('generator.pt'))\n",
        "discriminator.load_state_dict(torch.load('discriminator.pt'))\n",
        "transformer.load_state_dict(torch.load('transformer.pt'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WupqhdLHsAd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e448a8b4-7bd2-41e4-8f25-869a5fe6c6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#ADICIONADO\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "# Nome do modelo usado durante o treinamento\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Carregar tokenizer e configuração\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aLd8zgnRsLL5"
      },
      "outputs": [],
      "source": [
        "#ADICIONADO\n",
        "def preprocess(text, max_seq_length=64):\n",
        "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids = torch.tensor([encoded_sent])\n",
        "    attention_mask = torch.tensor([[int(token_id > 0) for token_id in encoded_sent]])\n",
        "    return input_ids, attention_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_1NDOhpZsMfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6519ac7-5334-4d42-d845-fe49259f2e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: HUM_hum\n"
          ]
        }
      ],
      "source": [
        "#ADICIONADO\n",
        "def classify_question(text):\n",
        "    transformer.eval()\n",
        "    discriminator.eval()\n",
        "\n",
        "    # Pré-processar a nova pergunta\n",
        "    input_ids, attention_mask = preprocess(text)\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Passar pela transformer\n",
        "        model_outputs = transformer(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "\n",
        "        # Passar pelo discriminator\n",
        "        _, logits, probs = discriminator(hidden_states)\n",
        "\n",
        "        # Filtrar os logits\n",
        "        filtered_logits = logits[:, :-1]\n",
        "\n",
        "        # Obter a predição\n",
        "        pred = torch.argmax(filtered_logits, dim=1).item()\n",
        "\n",
        "    return label_list[pred]\n",
        "\n",
        "# Exemplo de uso\n",
        "new_question = \"Who is the current president of the United States?\"\n",
        "predicted_label = classify_question(new_question)\n",
        "print(f\"Predicted Label: {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ori5FNHgruUz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "3a8cfe8d-5f4f-432b-8563-e97dfb36893a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import nltk\\nfrom nltk.tokenize import sent_tokenize\\n\\n# Baixar o recurso de pontuação da NLTK (apenas na primeira vez)\\nnltk.download(\"punkt\")\\n# Texto de entrada\\ntext = \"I’ve been asked to review a recent study on glyphosate’s cytotoxicity in rats. Given your expertise in the field, I believe it would be beneficial for both of you to act as peer reviewers. Once you’ve collated your comments, I’ll submit the final review on our behalf.\"\\n\\n# Dividir o texto em sentenças\\nsentences = sent_tokenize(text)\\n# Classificar cada sentença individualmente\\nfor sentence in sentences:\\n    predicted_label = classify_question(sentence)\\n    print(f\"Sentence: {sentence}\")\\n    print(f\"Predicted Label: {predicted_label}\\n\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "'''import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Baixar o recurso de pontuação da NLTK (apenas na primeira vez)\n",
        "nltk.download(\"punkt\")\n",
        "# Texto de entrada\n",
        "text = \"I’ve been asked to review a recent study on glyphosate’s cytotoxicity in rats. Given your expertise in the field, I believe it would be beneficial for both of you to act as peer reviewers. Once you’ve collated your comments, I’ll submit the final review on our behalf.\"\n",
        "\n",
        "# Dividir o texto em sentenças\n",
        "sentences = sent_tokenize(text)\n",
        "# Classificar cada sentença individualmente\n",
        "for sentence in sentences:\n",
        "    predicted_label = classify_question(sentence)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\\n\")'''\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}